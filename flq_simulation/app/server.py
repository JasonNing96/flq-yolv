"""
FLQ-Fed è”é‚¦å­¦ä¹ æœåŠ¡å™¨
ç®€åŒ–ç‰ˆ - é›†ä¸­æ‰€æœ‰æœåŠ¡å™¨é€»è¾‘
"""
import os
import torch
from pathlib import Path
from datetime import datetime
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import uvicorn

from .config import Config
from .model_utils import (
    state_dict_to_vector, vector_to_state_dict,
    fedavg_aggregate, compute_model_size, compute_compression_ratio,
    state_dict_to_grad_vector, grad_vector_to_state_dict,
    quantize_vector, dequantize_vector
)


# ==================== æ•°æ®æ¨¡å‹ ====================

class UpdateRequest(BaseModel):
    """å®¢æˆ·ç«¯ä¸Šä¼ æ›´æ–°çš„è¯·æ±‚"""
    client_id: int
    grad_vector: List[float]  # åºåˆ—åŒ–çš„æ¢¯åº¦å‘é‡
    n_samples: int
    round_id: int
    metrics: Optional[Dict[str, float]] = None  # è®­ç»ƒæŒ‡æ ‡ï¼ˆmAP, lossç­‰ï¼‰
    bits_up: Optional[float] = None # å®¢æˆ·ç«¯ä¸Šä¼ çš„æ¨¡å‹æ¯”ç‰¹æ•°
    quant_params: Optional[Dict[str, Any]] = None # é‡åŒ–å‚æ•° (scale, zero_point, bits)


class StatusResponse(BaseModel):
    """æœåŠ¡å™¨çŠ¶æ€å“åº”"""
    current_round: int
    total_rounds: int
    training_done: bool
    buffered_updates: int
    clients_per_round: int
    waiting_for: int
    aggregation_mode: str # æ–°å¢èšåˆæ¨¡å¼
    # è®­ç»ƒæŒ‡æ ‡
    avg_map50: Optional[float] = None
    avg_loss: Optional[float] = None
    round_time: Optional[float] = None
    bits_down_total_round: Optional[float] = None # æœåŠ¡å™¨ä¸‹å‘æ¨¡å‹æ€»æ¯”ç‰¹æ•°
    bits_up_total_round: Optional[float] = None # å®¢æˆ·ç«¯ä¸Šä¼ æ¨¡å‹æ€»æ¯”ç‰¹æ•°


# ==================== æœåŠ¡å™¨çŠ¶æ€ ====================

class ServerState:
    """æœåŠ¡å™¨å…¨å±€çŠ¶æ€"""
    
    def __init__(self, config: Config, initial_model):
        self.config = config
        self.model = initial_model
        self.global_state = initial_model.model.state_dict()
        self.last_global_state = initial_model.model.state_dict() # ç”¨äºFLQæ¨¡å¼ä¸‹è®¡ç®—æ¢¯åº¦å·®å¼‚

        # è®­ç»ƒçŠ¶æ€
        self.current_round = 0
        self.training_done = False

        # ç¼“å†²åŒº
        self.update_buffer = []
        self.sample_counts = []
        self.metrics_buffer = []  # å­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯çš„æŒ‡æ ‡
        self.bits_up_buffer = [] # å­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯ä¸Šä¼ çš„æ¯”ç‰¹æ•°

        # ç»Ÿè®¡ä¿¡æ¯
        self.round_start_time = None
        self.round_metrics = {}  # æ¯è½®çš„å¹³å‡æŒ‡æ ‡
        self.total_params, self.model_size_mb = compute_model_size(self.global_state, 32)
        self.bits_down_per_round = 0 # æœåŠ¡å™¨ä¸‹å‘æ¨¡å‹å¤§å°ï¼ˆæ¯”ç‰¹ï¼‰

        print(f"[{self._ts()}] ğŸ“¦ æ¨¡å‹å‚æ•°: {self.total_params:,} ({self.model_size_mb:.1f} MB)")
        print(f"[{self._ts()}] ğŸ¯ è®­ç»ƒç›®æ ‡: {config.rounds} è½® Ã— {config.clients_per_round} å®¢æˆ·ç«¯")
    
    def _ts(self):
        """æ—¶é—´æˆ³"""
        return datetime.now().strftime('%H:%M:%S')
    
    def add_update(self, grad_vector: torch.Tensor, n_samples: int, metrics: Optional[Dict] = None, bits_up: Optional[float] = None, quant_params: Optional[Dict] = None):
        """æ·»åŠ å®¢æˆ·ç«¯æ›´æ–°åˆ°ç¼“å†²åŒº"""
        self.update_buffer.append({'grad_vector': grad_vector, 'quant_params': quant_params})
        self.sample_counts.append(n_samples)
        if metrics:
            self.metrics_buffer.append(metrics)
        if bits_up is not None:
            self.bits_up_buffer.append(bits_up)

        waiting = self.config.clients_per_round - len(self.update_buffer)
        print(f"[{self._ts()}] ğŸ“¥ æ”¶åˆ°å®¢æˆ·ç«¯æ›´æ–° ({len(self.update_buffer)}/{self.config.clients_per_round})")

        if len(self.update_buffer) >= self.config.clients_per_round:
            self._aggregate_and_advance()
    
    def _aggregate_and_advance(self):
        """èšåˆæ›´æ–°å¹¶æ¨è¿›åˆ°ä¸‹ä¸€è½®"""
        print(f"\n{'='*70}")
        print(f"[{self._ts()}] ğŸ”„ èšåˆ Round {self.current_round} (Mode: {self.config.aggregation_mode})")
        print(f"{'='*70}")

        aggregated_grad_vector = None
        if self.config.aggregation_mode == "fedavg":
            # FedAvg èšåˆ state_dict
            state_dict_updates = [item['grad_vector'] for item in self.update_buffer] # è¿™é‡Œçš„grad_vectorå®é™…ä¸Šæ˜¯state_dict
            self.global_state = fedavg_aggregate(state_dict_updates, self.sample_counts)
        elif self.config.aggregation_mode == "flq-fed":
            # FLQ-Fed èšåˆæ¢¯åº¦å‘é‡
            grad_vectors = []
            for item in self.update_buffer:
                grad_vec = torch.tensor(item['grad_vector'])
                quant_params = item['quant_params']
                
                if quant_params:
                    # åé‡åŒ–
                    dequantized_grad_vec = dequantize_vector(
                        grad_vec,
                        quant_params['scale'],
                        quant_params['zero_point'],
                        quant_params['bits']
                    )
                    grad_vectors.append(dequantized_grad_vec)
                else:
                    grad_vectors.append(grad_vec) # å¦‚æœæ²¡æœ‰é‡åŒ–å‚æ•°ï¼Œè¯´æ˜æ˜¯å…¨ç²¾åº¦æ¢¯åº¦
            
            aggregated_grad_vector = fedavg_aggregate(grad_vectors, self.sample_counts)
            self.global_state = grad_vector_to_state_dict(aggregated_grad_vector, self.last_global_state)
        else:
            raise ValueError(f"æœªçŸ¥çš„èšåˆæ¨¡å¼: {self.config.aggregation_mode}")

        # æ›´æ–° last_global_state
        self.last_global_state = self.global_state

        # è®¡ç®—æœåŠ¡å™¨ä¸‹å‘æ¨¡å‹å¤§å°
        _, bits_down_mb = compute_model_size(self.global_state, bits=32) # å‡è®¾æ˜¯32bitæµ®ç‚¹æ•°
        self.bits_down_per_round = bits_down_mb * (1024 ** 2) * 8 # è½¬æ¢ä¸ºæ¯”ç‰¹

        # èšåˆå®¢æˆ·ç«¯æŒ‡æ ‡å’Œé€šä¿¡é‡
        round_metrics = {}
        if self.metrics_buffer:
            for key in self.metrics_buffer[0].keys():
                values = [m.get(key, 0.0) for m in self.metrics_buffer]
                round_metrics[key] = sum(values) / len(values)
        
        if self.bits_up_buffer:
            round_metrics['bits_up_total_round'] = sum(self.bits_up_buffer)
        round_metrics['bits_down_total_round'] = self.bits_down_per_round

        self.round_metrics[self.current_round] = round_metrics
        print(f"ğŸ“Š å¹³å‡æŒ‡æ ‡: mAP50={round_metrics.get('map50', 0.0):.4f}, Loss={round_metrics.get('loss', 0.0):.4f}")
        print(f"â¬†ï¸  æœ¬è½®ä¸Šä¼ æ€»æ¯”ç‰¹: {round_metrics.get('bits_up_total_round', 0.0):.2f}")
        print(f"â¬‡ï¸  æœ¬è½®ä¸‹å‘æ€»æ¯”ç‰¹: {round_metrics.get('bits_down_total_round', 0.0):.2f}")

        # ä¿å­˜checkpoint
        save_dir = Path(self.config.server_save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_path = save_dir / f"global_round_{self.current_round + 1}.pt"
        torch.save(self.global_state, checkpoint_path)

        # ç»Ÿè®¡ä¿¡æ¯
        round_time = (datetime.now() - self.round_start_time).total_seconds() if self.round_start_time else 0
        compress_ratio = compute_compression_ratio(32, self.config.quant_bits if self.config.quant_enabled else 32)

        print(f"â±ï¸  è½®æ¬¡æ—¶é—´: {round_time:.1f}s")
        print(f"ğŸ’¾ Checkpoint: {checkpoint_path}")
        print(f"ğŸ—œï¸  å‹ç¼©ç‡: {compress_ratio:.2f}x")
        print(f"{'='*70}\n")

        # æ¸…ç©ºç¼“å†²åŒºå¹¶æ¨è¿›
        self.update_buffer.clear()
        self.sample_counts.clear()
        self.metrics_buffer.clear()
        self.bits_up_buffer.clear()
        self.current_round += 1
        self.round_start_time = datetime.now()

        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        if self.current_round >= self.config.rounds:
            self.training_done = True
            print(f"ğŸ‰ æ‰€æœ‰è®­ç»ƒè½®æ¬¡å·²å®Œæˆï¼")
            self._save_metrics_to_csv()
    
    def get_global_model(self) -> tuple:
        """è·å–å…¨å±€æ¨¡å‹"""
        return self.global_state, self.current_round, self.training_done

    def _save_metrics_to_csv(self):
        """ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°CSVæ–‡ä»¶"""
        import csv

        csv_path = Path(self.config.server_save_dir) / "training_metrics.csv"
        csv_path.parent.mkdir(parents=True, exist_ok=True)

        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            # åŠ¨æ€ç”Ÿæˆè¡¨å¤´
            if not self.round_metrics:
                writer.writerow(['round'])
                return
            
            first_round_metrics = next(iter(self.round_metrics.values()))
            fieldnames = ['round'] + sorted(first_round_metrics.keys())
            writer.writerow(fieldnames)

            for round_id, metrics in self.round_metrics.items():
                row = [round_id] + [metrics.get(key, '') for key in sorted(first_round_metrics.keys())]
                writer.writerow(row)

        print(f"\nğŸ“Š è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜åˆ°: {csv_path}")

    def get_current_metrics(self) -> Dict:
        """è·å–å½“å‰è½®æ¬¡çš„æŒ‡æ ‡"""
        if self.current_round > 0 and (self.current_round - 1) in self.round_metrics:
            return self.round_metrics[self.current_round - 1]
        return {}


# ==================== FastAPI åº”ç”¨ ====================

def create_app(config: Config, initial_model) -> FastAPI:
    """åˆ›å»º FastAPI åº”ç”¨"""
    
    app = FastAPI(title="FLQ-Fed Server")
    state = ServerState(config, initial_model)
    
    @app.get("/")
    def root():
        return {"message": "FLQ-Fed Server", "version": "2.0-simplified"}
    
    @app.get("/status", response_model=StatusResponse)
    def get_status():
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        current_metrics = state.get_current_metrics()
        round_time = (datetime.now() - state.round_start_time).total_seconds() if state.round_start_time else 0.0

        return StatusResponse(
            current_round=state.current_round,
            total_rounds=state.config.rounds,
            training_done=state.training_done,
            buffered_updates=len(state.update_buffer),
            clients_per_round=state.config.clients_per_round,
            waiting_for=state.config.clients_per_round - len(state.update_buffer),
            aggregation_mode=state.config.aggregation_mode,
            avg_map50=current_metrics.get('map50'),
            avg_loss=current_metrics.get('loss'),
            round_time=round_time,
            bits_down_total_round=current_metrics.get('bits_down_total_round'),
            bits_up_total_round=current_metrics.get('bits_up_total_round')
        )
    
    @app.get("/global")
    def get_global():
        """å®¢æˆ·ç«¯æ‹‰å–å…¨å±€æ¨¡å‹"""
        global_state, round_id, done = state.get_global_model()
        
        # å¦‚æœå¯ç”¨ä¸‹è¡Œé‡åŒ–
        if state.config.downlink_quant_bits > 0:
            # å°† state_dict è½¬æ¢ä¸ºå‘é‡
            global_vector = state_dict_to_vector(global_state)
            
            # é‡åŒ–
            quantized_vector, scale, zero_point = quantize_vector(
                global_vector, bits=state.config.downlink_quant_bits
            )
            
            # åé‡åŒ–å›å…¨ç²¾åº¦ï¼Œä»¥ä¾¿å®¢æˆ·ç«¯ç›´æ¥åŠ è½½
            dequantized_vector = dequantize_vector(
                quantized_vector, scale, zero_point, bits=state.config.downlink_quant_bits
            )
            
            # è½¬æ¢å› state_dict
            global_state = vector_to_state_dict(dequantized_vector, global_state)
            
            # è®¡ç®—ä¸‹å‘æ¨¡å‹å¤§å°ï¼ˆé‡åŒ–åï¼‰
            _, bits_down_mb = compute_model_size(global_state, bits=state.config.downlink_quant_bits)
            state.bits_down_per_round = bits_down_mb * (1024 ** 2) * 8
        else:
            # å¦åˆ™ï¼ŒæŒ‰32bitè®¡ç®—
            _, bits_down_mb = compute_model_size(global_state, bits=32)
            state.bits_down_per_round = bits_down_mb * (1024 ** 2) * 8

        # åºåˆ—åŒ– state_dict
        serialized = {k: v.cpu().tolist() for k, v in global_state.items()}
        
        return {
            "state_dict": serialized,
            "round": round_id,
            "done": done,
            "downlink_quant_bits": state.config.downlink_quant_bits # å‘ŠçŸ¥å®¢æˆ·ç«¯ä¸‹è¡Œé‡åŒ–æ¯”ç‰¹æ•°
        }
    
    @app.post("/update")
    def receive_update(request: UpdateRequest):
        """æ¥æ”¶å®¢æˆ·ç«¯æ›´æ–°"""
        if state.training_done:
            return {"success": True, "message": "è®­ç»ƒå·²å®Œæˆ", "done": True}

        # ååºåˆ—åŒ– grad_vector
        grad_vector = torch.tensor(request.grad_vector)

        # æ·»åŠ æ›´æ–°ï¼ˆåŒ…æ‹¬æŒ‡æ ‡å’Œæ¯”ç‰¹æ•°ï¼‰
        state.add_update(
            grad_vector,
            request.n_samples,
            request.metrics,
            request.bits_up,
            request.quant_params
        )

        return {
            "success": True,
            "round": state.current_round,
            "done": state.training_done,
            "buffered": len(state.update_buffer)
        }
    
    return app


# ==================== å¯åŠ¨å‡½æ•° ====================

def start_server(config_path: Optional[str] = None):
    """
    å¯åŠ¨è”é‚¦å­¦ä¹ æœåŠ¡å™¨
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    print("="*70)
    print("ğŸš€ FLQ-Fed è”é‚¦å­¦ä¹ æœåŠ¡å™¨")
    print("="*70)
    
    # åŠ è½½é…ç½®
    config = Config(config_path)
    print(f"âœ… é…ç½®: {config}\n")
    
    # åˆå§‹åŒ–æ¨¡å‹
    from ultralytics import YOLO
    project_root = Path(__file__).parent.parent
    model_path = project_root / config.model_name
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    model = YOLO(str(model_path))
    
    # åˆå§‹åŒ–ç±»åˆ«æ•°ï¼ˆä»æ•°æ®é…ç½®è¯»å–ï¼‰
    data_yaml = project_root / "data" / "client1" / "oil.yaml"
    if data_yaml.exists():
        import yaml
        with open(data_yaml) as f:
            data_cfg = yaml.safe_load(f)
        nc = data_cfg.get('nc', 80)
        
        from ultralytics.nn.tasks import DetectionModel
        model.model = DetectionModel(model.model.yaml, ch=3, nc=nc)
        print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ (nc={nc})\n")
    
    # åˆ›å»º FastAPI åº”ç”¨
    app = create_app(config, model)
    
    # å¯åŠ¨æœåŠ¡å™¨
    print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸŒ å¯åŠ¨æœåŠ¡å™¨: http://{config.server_host}:{config.server_port}")
    print(f"{'='*70}\n")
    
    uvicorn.run(
        app,
        host=config.server_host,
        port=config.server_port,
        log_level="warning"
    )


if __name__ == "__main__":
    start_server()

